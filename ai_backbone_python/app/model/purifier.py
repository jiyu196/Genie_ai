# app/model/purifier.py
import re
import torch
import logging
import sys
from app.model.model_loader import tokenizer, model, DEVICE, MAX_INPUT_LEN

logger = logging.getLogger("purifier")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter(
        "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.propagate = False


def remove_dummy_tokens(text: str) -> str:
    return re.sub(r"(ê·¸ê·¸ê·¸|ìœ¼ìœ¼)", "", text or "")

def keep_before_first_period(text: str) -> str:
    if not text:
        return " "
    pos = text.find(".")
    return text if pos == -1 else text[:pos]

def purify_sentence(text: str) -> str:
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_INPUT_LEN
    )
    inputs.pop("token_type_ids", None)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=64,
            do_sample=False,
            num_beams=1,
            repetition_penalty=2.0,
            no_repeat_ngram_size=4,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)



def refine(text: str) -> str:
    """
    âœ… Pchange=trueì¼ ë•Œ í˜¸ì¶œë˜ëŠ” ì§„ìž…ì 
    - ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ë¥¼ ë¡œê·¸ë¡œ ëª…í™•ížˆ ë‚¨ê¹€
    - ìˆœí™” ì „ / ìˆœí™” í›„ ë¬¸ìž¥ì„ ëª¨ë‘ ì¶œë ¥
    """
    print("ðŸ”¥ purifier module loaded")
    original_text = text

    try:
        # ðŸ”¥ ëª¨ë¸ í†µê³¼
        purified = purify_sentence(text)

        # ðŸ”§ í›„ì²˜ë¦¬
        purified = remove_dummy_tokens(purified)
        purified = keep_before_first_period(purified)

        # âœ… ë¡œê·¸ ì¶œë ¥ (í•µì‹¬)
        logger.info(
            "[PURIFIER] model_used=True | before='%s' | after='%s'",
            original_text,
            purified
        )

        return purified

    except Exception as e:
        # âŒ ëª¨ë¸ ë¯¸ì‚¬ìš© (fallback)
        logger.warning(
            "[PURIFIER] model_used=False | before='%s' | reason=%s",
            original_text,
            str(e)
        )
        return original_text
# def refine(text: str) -> str:
#     try:
#         result = purify_sentence(text)
#         result = remove_dummy_tokens(result)
#         result = keep_before_first_period(result)
#         return result
#     except Exception:
#         return text
